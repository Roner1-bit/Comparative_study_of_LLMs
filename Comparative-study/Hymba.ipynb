{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbYJDN650yea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Set directories and model IDs\n",
        "hymba_model_id = \"nvidia/Hymba-1.5B-Instruct\"\n",
        "blip_model_id = \"Salesforce/blip-image-captioning-base\"\n",
        "cases_dir = '/path/to/cases/'\n",
        "\n",
        "# Load models and processors\n",
        "tokenizer_hymba = AutoTokenizer.from_pretrained(hymba_model_id, trust_remote_code=True)\n",
        "model_hymba = AutoModelForCausalLM.from_pretrained(hymba_model_id, trust_remote_code=True).to('cuda').to(torch.bfloat16)\n",
        "\n",
        "processor_blip = BlipProcessor.from_pretrained(blip_model_id)\n",
        "model_blip = BlipForConditionalGeneration.from_pretrained(blip_model_id).to('cuda')\n",
        "\n",
        "# Helper functions\n",
        "def generate_caption(image, processor, model):\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to('cuda')\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "def load_images_from_dir(image_dir):\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    images = []\n",
        "    for img_file in image_files:\n",
        "        img_path = os.path.join(image_dir, img_file)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        images.append(img)\n",
        "    return images\n",
        "\n",
        "# Iterate through cases\n",
        "for case in os.listdir(cases_dir):\n",
        "    case_dir = os.path.join(cases_dir, case)\n",
        "    if not os.path.isdir(case_dir):\n",
        "        continue  # skip non-directories\n",
        "\n",
        "    clinical_info_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
        "    if not os.path.exists(clinical_info_path):\n",
        "        print(f\"Missing clinical info for case {case}\")\n",
        "        continue\n",
        "\n",
        "    clinical_info = open(clinical_info_path, 'r', encoding='utf-8').read()\n",
        "\n",
        "    image_dir = os.path.join(case_dir, 'images')\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"No images found for case {case}\")\n",
        "        continue\n",
        "\n",
        "    images = load_images_from_dir(image_dir)\n",
        "    if not images:\n",
        "        print(f\"No images loaded for case {case}\")\n",
        "        continue\n",
        "\n",
        "    # Generate captions for each image\n",
        "    captions = []\n",
        "    for img in images:\n",
        "        caption = generate_caption(img, processor_blip, model_blip)\n",
        "        captions.append(caption)\n",
        "\n",
        "    # Combine clinical info and image captions\n",
        "    prompt = f\"\"\"Clinical data: {clinical_info}\n",
        "\n",
        "    Image descriptions:\n",
        "    \"\"\"\n",
        "    for i, cap in enumerate(captions, 1):\n",
        "        prompt += f\"Image {i}: {cap}\\n\"\n",
        "\n",
        "    prompt += \"\\nPlease provide a detailed diagnosis report based on the clinical data and images.\"\n",
        "\n",
        "    # Prepare messages for Hymba\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a professional radiologist providing detailed diagnosis reports.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer_hymba.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        outputs = model_hymba.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=4096,\n",
        "            do_sample=False,\n",
        "            temperature=0.7,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "    # Decode the response\n",
        "    response = tokenizer_hymba.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Save the response\n",
        "    response_path = os.path.join(case_dir, 'hymba-report.txt')\n",
        "    with open(response_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response)\n",
        "\n",
        "    print(f\"Report generated for case {case}.\")"
      ]
    }
  ]
}