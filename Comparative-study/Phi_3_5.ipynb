{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKonH-1yzLhD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "# Set model ID and directories\n",
        "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
        "cases_dir = '/media/elboardy/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized (Copy)/'\n",
        "\n",
        "# Load the Phi-3.5-vision-instruct model and processor\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=\"auto\",\n",
        "    _attn_implementation='flash_attention_2'\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    num_crops=4\n",
        ")\n",
        "\n",
        "# Function to load images from directory\n",
        "def load_images_from_dir(image_dir):\n",
        "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith('.jpg') or f.lower().endswith('.png')]\n",
        "    images = []\n",
        "    for i, image_file in enumerate(image_files, start=1):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        images.append(Image.open(image_path))\n",
        "    return images\n",
        "\n",
        "# Iterate through cases in the directory\n",
        "for case in os.listdir(cases_dir):\n",
        "    case_dir = os.path.join(cases_dir, case)\n",
        "    if not os.path.isdir(case_dir):\n",
        "        continue  # Skip non-directory files\n",
        "\n",
        "    image_dir = os.path.join(case_dir, 'images')  # Assuming images are in a subdirectory named 'images'\n",
        "    if not os.path.exists(image_dir):\n",
        "        print(f\"No images found for case: {case}\")\n",
        "        continue\n",
        "\n",
        "    clinical_information_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
        "    if not os.path.exists(clinical_information_path):\n",
        "        print(f\"Missing clinical information file for case: {case}\")\n",
        "        continue\n",
        "\n",
        "    clinical_information = open(clinical_information_path, 'r', encoding='utf-8').read()\n",
        "\n",
        "    # Define prompts\n",
        "    system_prompt = \"\"\"Consider that you are a professional radiologist with several years of experience. Write a detailed diagnosis report for the provided images and clinical data.\"\"\"\n",
        "    user_prompt = f\"\"\"Clinical data: {clinical_information}\\n\\nPlease provide a diagnosis report based on the images and clinical data.\"\"\"\n",
        "\n",
        "    # Load images\n",
        "    images = load_images_from_dir(image_dir)\n",
        "\n",
        "    # Prepare placeholders for images\n",
        "    placeholder = \"\"\n",
        "    for i in range(1, len(images) + 1):\n",
        "        placeholder += f\"<|image_{i}|>\\n\"\n",
        "\n",
        "    # Create messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": placeholder + user_prompt},\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    prompt = processor.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    # Generate response\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 1000,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False,\n",
        "    }\n",
        "    generate_ids = model.generate(\n",
        "        **inputs,\n",
        "        eos_token_id=processor.tokenizer.eos_token_id,\n",
        "        **generation_args\n",
        "    )\n",
        "\n",
        "    # Remove input tokens\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "\n",
        "    # Decode response\n",
        "    response = processor.batch_decode(\n",
        "        generate_ids,\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    # Save the response\n",
        "    response_path = os.path.join(case_dir, 'phi-3.5-vision-instruct-response.txt')\n",
        "    with open(response_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response)\n",
        "\n",
        "    print(f\"Response saved for case {case}.\")"
      ]
    }
  ]
}