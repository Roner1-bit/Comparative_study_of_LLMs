{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSMBJLK1t3hX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import torch\n",
        "\n",
        "# Set model ID and directories\n",
        "model_id = \"Qwen/Qwen2-VL-72B-Instruct\"\n",
        "cases_dir = '/media/elboardy/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized (Copy)/'\n",
        "\n",
        "# Load the Qwen2-VL model and processor\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=\"auto\",  # Adjust if needed\n",
        "    device_map=\"auto\",\n",
        "    cache_dir='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
        "    offload_folder='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Function to load images\n",
        "def load_image(image_path):\n",
        "    return Image.open(image_path)\n",
        "\n",
        "# Iterate through cases in the directory\n",
        "for case in os.listdir(cases_dir):\n",
        "    case_dir = os.path.join(cases_dir, case)\n",
        "    image_files = [\n",
        "        f for f in os.listdir(case_dir)\n",
        "        if f.lower().endswith('.png')\n",
        "    ]\n",
        "\n",
        "    clinical_information_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
        "    if not os.path.exists(clinical_information_path):\n",
        "        print(f\"Missing clinical information file for case: {case}\")\n",
        "        continue\n",
        "\n",
        "    clinical_information = open(clinical_information_path).read()\n",
        "\n",
        "    # Define prompts\n",
        "    system_prompt = \"\"\"Consider that you are a professional radiologist with several years of experience. Write a detailed diagnosis report for the provided images and clinical data.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"You will be given batches of MRI images for patients likely to have brain tumors. Each batch includes sequences and segmentation masks.\n",
        "    Clinical data:\n",
        "    {clinical_information}\"\"\"\n",
        "\n",
        "    # Load images\n",
        "    images = []\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(case_dir, image_file)\n",
        "        images.append({\"type\": \"image\", \"image\": load_image(image_path)})\n",
        "\n",
        "    # Prepare messages for Qwen2-VL\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_prompt}] + images},\n",
        "    ]\n",
        "\n",
        "    # Prepare inputs for Qwen2-VL\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate the response\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=4096)\n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )[0]\n",
        "\n",
        "    # Save the response\n",
        "    response_path = os.path.join(case_dir, 'qwen-vl-72b-response.txt')\n",
        "    with open(response_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response_text)\n",
        "\n",
        "    print(f\"Response saved for case {case}.\")\n"
      ]
    }
  ]
}