{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8hGvh3tqxl_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
        "import torch\n",
        "\n",
        "# Set model ID and directories\n",
        "model_id = \"Intel/llava-gemma-7b\"\n",
        "cases_dir = '/media/elboardy/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized (Copy)/'\n",
        "\n",
        "# Load the Llava-Gemma model and processor\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # Use appropriate dtype based on hardware\n",
        "    device_map=\"auto\",\n",
        "    cache_dir='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
        "    offload_folder='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load an image\n",
        "def load_image(image_path):\n",
        "    return Image.open(image_path)\n",
        "\n",
        "# Iterate through cases in the provided directory\n",
        "for case in os.listdir(cases_dir):\n",
        "    case_dir = os.path.join(cases_dir, case)\n",
        "    image_files = [\n",
        "        f for f in os.listdir(case_dir)\n",
        "        if f.lower().endswith('.png')\n",
        "    ]\n",
        "\n",
        "    clinical_information_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
        "    if not os.path.exists(clinical_information_path):\n",
        "        print(f\"Missing clinical information file for case: {case}\")\n",
        "        continue\n",
        "\n",
        "    clinical_information = open(clinical_information_path).read()\n",
        "\n",
        "    # Define prompts\n",
        "    system_prompt = \"\"\"Consider that you are a professional radiologist with several years of experience. Write a detailed diagnosis report for the provided images and clinical data.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"You will be given batches of MRI images for patients likely to have brain tumors. Each batch includes sequences and segmentation masks.\n",
        "    Clinical data:\n",
        "    {clinical_information}\"\"\"\n",
        "\n",
        "    # Load images\n",
        "    images = []\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(case_dir, image_file)\n",
        "        images.append(load_image(image_path))\n",
        "\n",
        "    # Prepare the messages for Llava-Gemma\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template\n",
        "    input_text = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(\n",
        "        images=images,\n",
        "        text=input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate the response\n",
        "    output = model.generate(**inputs, max_new_tokens=4096)\n",
        "    response_text = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Save the response\n",
        "    response_path = os.path.join(case_dir, 'llava-gemma-response-7b.txt')\n",
        "    with open(response_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(response_text)\n",
        "\n",
        "    print(f\"Response saved for case {case}.\")"
      ],
      "metadata": {
        "id": "eHykvc1-rtkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}