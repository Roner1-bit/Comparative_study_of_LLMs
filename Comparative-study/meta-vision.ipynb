{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elboardy/anaconda3/envs/meta-vision/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/elboardy/anaconda3/envs/meta-vision/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in using token...\n",
      "Login successful using token!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "\n",
    "HF_TOKEN = \"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\"  \n",
    "print(\"Logging in using token...\")\n",
    "try:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"Login successful using token!\")\n",
    "except Exception as e:\n",
    "    print(f\"Login failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.set_per_process_memory_fraction(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "/home/elboardy/anaconda3/envs/meta-vision/lib/python3.9/site-packages/accelerate/utils/modeling.py:1593: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1dd50471a3431a8e3e10710b65383e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-90B-Vision\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
    "    offload_folder='/media/elboardy/RLAB-Disk01/Large-Language-Models-Weights',\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "prompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=4096)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deepspeed\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # Initialize model and tokenizer\n",
    "# model_name = 'bert-large-uncased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# # DeepSpeed inference initialization\n",
    "# ds_engine = deepspeed.init_inference(\n",
    "#     model,\n",
    "#     mp_size=1,\n",
    "#     dtype=torch.float16,\n",
    "#     replace_method='auto',\n",
    "#     replace_with_kernel_inject=True\n",
    "# )\n",
    "\n",
    "# # Prepare input\n",
    "# input_text = \"This is a sample input.\"\n",
    "# inputs = tokenizer(input_text, return_tensors='pt').to('cuda')\n",
    "\n",
    "# # Perform inference\n",
    "# with torch.no_grad():\n",
    "#     outputs = ds_engine.module(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
