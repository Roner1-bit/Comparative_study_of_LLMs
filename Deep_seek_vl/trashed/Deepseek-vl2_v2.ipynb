{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elboardy/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/elboardy/.local/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/elboardy/.local/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoModelForCausalLM\n",
    "from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM\n",
    "from deepseek_vl2.utils.io import load_pil_images\n",
    "import torch\n",
    "import gc\n",
    "import traceback\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_dir = '/media/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized/'\n",
    "cache_dir = '/media/RLAB-Disk01/Large-Language-Models-Weights'\n",
    "model_id = \"deepseek-ai/deepseek-vl2\"\n",
    "offload_folder = '/media/RLAB-Disk01/Large-Language-Models-Weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# torch.set_float32_matmul_precision('medium')\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF']=\"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Make sure cases_dir is properly defined\n",
    "# # cases_dir = \"your/cases/directory/path\"\n",
    "\n",
    "# deleted_count = 0\n",
    "# for case in os.listdir(cases_dir):\n",
    "#     case_dir = os.path.join(cases_dir, case)\n",
    "#     if os.path.isdir(case_dir):\n",
    "#         response_path = os.path.join(case_dir, 'deepseek-vl-small.txt')\n",
    "#         if os.path.exists(response_path):\n",
    "#             try:\n",
    "#                 os.remove(response_path)\n",
    "#                 deleted_count += 1\n",
    "#                 print(f\"Deleted: {response_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error deleting {response_path}: {str(e)}\")\n",
    "\n",
    "# print(f\"\\nDeleted {deleted_count} response files.\")\n",
    "# print(f\"Remaining cases without response file: {len(os.listdir(cases_dir)) - deleted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 cases to process.\n"
     ]
    }
   ],
   "source": [
    "cases_to_process = []\n",
    "for case in os.listdir(cases_dir):\n",
    "    case_dir = os.path.join(cases_dir, case)\n",
    "    if os.path.isdir(case_dir):\n",
    "        response_path = os.path.join(case_dir, 'deepseek-vl2.txt')\n",
    "        if not os.path.exists(response_path):\n",
    "            cases_to_process.append(case)\n",
    "print(f\"Found {len(cases_to_process)} cases to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elboardy/.local/lib/python3.9/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "/home/elboardy/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add pad token = ['<｜▁pad▁｜>'] to the tokenizer\n",
      "<｜▁pad▁｜>:2\n",
      "Add image token = ['<image>'] to the tokenizer\n",
      "<image>:128815\n",
      "Add grounding-related tokens = ['<|ref|>', '<|/ref|>', '<|det|>', '<|/det|>', '<|grounding|>'] to the tokenizer with input_ids\n",
      "<|ref|>:128816\n",
      "<|/ref|>:128817\n",
      "<|det|>:128818\n",
      "<|/det|>:128819\n",
      "<|grounding|>:128820\n",
      "Add chat tokens = ['<|User|>', '<|Assistant|>'] to the tokenizer with input_ids\n",
      "<|User|>:128821\n",
      "<|Assistant|>:128822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.set_default_tensor_type(torch.FloatTensor) \n",
    "\n",
    "processor = DeepseekVLV2Processor.from_pretrained(model_id, torch_dtype=torch.float16,\n",
    "                                                 device_map=\"cpu\", trust_remote_code=True, \n",
    "                                                 cache_dir=cache_dir, \n",
    "                                                 offload_folder=offload_folder)\n",
    "tokenizer = processor.tokenizer\n",
    "model: DeepseekVLV2ForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=cache_dir,\n",
    "    offload_folder=offload_folder,\n",
    "    # attn_implementation=\"eager\",\n",
    "    \n",
    ")\n",
    "\n",
    "# model.to(torch.float32)\n",
    "\n",
    "\n",
    "# model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conversation(case_dir, user_prompt,system_prompt):\n",
    "    image_files = [f for f in os.listdir(case_dir) if f.lower().endswith('.png')]\n",
    "    image_paths = [os.path.join(case_dir, f) for f in image_files]\n",
    "    \n",
    "    image_files= [image_files[-1]]\n",
    "    image_paths = [image_paths[-1]]\n",
    "    # Create image placeholders\n",
    "    print(image_files)\n",
    "    print(image_paths)\n",
    "    image_tags = \" \".join([\"<image>\"] * len(image_files))\n",
    "    print(image_tags)\n",
    "    \n",
    "    return [\n",
    "\n",
    "         {\n",
    "            \"role\": \"<|User|>\",\n",
    "            \"content\": f\"{image_tags}\\n<|ref|>{system_prompt}\\n{user_prompt}<|/ref|>\",\n",
    "            \"images\": image_paths\n",
    "        },\n",
    "\n",
    "        {\"role\": \"<|Assistant|>\", \"content\": \"\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_cases = []\n",
    "\n",
    "\n",
    "# for case in cases_to_process:\n",
    "#     case_dir = os.path.join(cases_dir, case)\n",
    "#     try:\n",
    "#         # Load clinical info\n",
    "#         clinical_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
    "#         if not os.path.exists(clinical_path):\n",
    "#             print(f\"Missing clinical info for {case}\")\n",
    "#             failed_cases.append(case)\n",
    "#             continue\n",
    "            \n",
    "#         clinical_info = open(clinical_path).read()\n",
    "        \n",
    "\n",
    "#         system_prompt = \"\"\"Consider that you are a professional radiologist with several years of experience and you are now treating a patient. Write a fully detailed diagnosis report for this case, avoiding any potential hallucination and paying close attention to all of the batch images attached to this message.\n",
    "\n",
    "# Use the following structure for the report:\n",
    "\n",
    "# ## Radiologist's Report\n",
    "\n",
    "# ### Patient Information:\n",
    "# - *Age:* 65\n",
    "# - *Sex:* Male\n",
    "# - *Days from earliest imaging to surgery:* 1\n",
    "# - *Histopathological Subtype:* Glioblastoma\n",
    "# - *WHO Grade:* 4\n",
    "# - *IDH Status:* Mutant\n",
    "# - *Preoperative KPS:* 80\n",
    "# - *Preoperative Contrast-Enhancing Tumor Volume (cm³):* 103.21\n",
    "# - *Preoperative T2/FLAIR Abnormality (cm³):* 36.29\n",
    "# - *Extent of Resection (EOR %):* 100.0\n",
    "# - *EOR Type:* Gross Total Resection (GTR)\n",
    "# - *Adjuvant Therapy:* Radiotherapy (RT) + Temozolomide (TMZ)\n",
    "# - *Progression-Free Survival (PFS) Days:* 649\n",
    "# - *Overall Survival (OS) Days:* 736\n",
    "\n",
    "# #### Tumor Characteristics:\n",
    "\n",
    "# #### Segmentation Analysis:\n",
    "\n",
    "# #### Surgical Considerations:\n",
    "\n",
    "# ### Clinical Summary:\n",
    "\n",
    "# ### Recommendations:\n",
    "\n",
    "# ### Prognostic Considerations:\n",
    "\n",
    "# ### Follow-Up Plan:\n",
    "\n",
    "# ### Additional Notes*(if any)*:\n",
    "\n",
    "# Ensure all findings from all of the images and clinical data provided. Please mention at the end of the report how many images were reviewed.\"\"\"\n",
    "\n",
    "#         user_prompt = f\"\"\"You will be given batches of images, which are different sequences of MRI scans. \n",
    "#     The images are for patients who are likely to have a brain tumor. Each image will contain up to 10 slices for 5 different sequences and the segmentation masks for the tumor at the bottom row of the image. \n",
    "#     Additional clinical data about the patient is: \n",
    "#     {clinical_info}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#         conversation = build_conversation(case_dir, user_prompt,system_prompt)\n",
    "#         pil_images = load_pil_images(conversation)\n",
    "        \n",
    "#         conversation = build_conversation(case_dir, user_prompt,system_prompt)\n",
    "#         pil_images = load_pil_images(conversation)\n",
    "        \n",
    "#         print(conversation)\n",
    "\n",
    "#         # Process inputs\n",
    "#         inputs = processor(\n",
    "#             conversations=conversation,\n",
    "#             images=pil_images,\n",
    "#             force_batchify=True,\n",
    "#             system_prompt=system_prompt\n",
    "#         ).to(model.device)\n",
    "\n",
    "#         # Generate embeddings\n",
    "#         inputs_embeds = model.prepare_inputs_embeds(**inputs)\n",
    "\n",
    "#         print(f\"Messages prepared for case {case}\")\n",
    "#         # Generate response\n",
    "\n",
    "\n",
    "\n",
    "#         outputs = model.language.generate(\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             attention_mask=inputs.attention_mask,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             bos_token_id=tokenizer.bos_token_id,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             max_new_tokens=4096,\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.9,\n",
    "#             do_sample=False,\n",
    "#             use_cache=True,\n",
    "#         )\n",
    "        \n",
    "#         # Decode and save response\n",
    "\n",
    "#         print(outputs.cpu().tolist())  \n",
    "#         response = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "#         print(f\"Response generated for case {case}, length: {len(response)} characters\")\n",
    "\n",
    "#         print(response)\n",
    "\n",
    "#         del inputs, outputs, pil_images, conversation, inputs_embeds\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model._clear_cuda_cache()\n",
    "\n",
    "#         with open(os.path.join(case_dir, 'deepseek-vl2.txt'), 'w', encoding='utf-8') as f:\n",
    "#             f.write(response.strip())\n",
    "\n",
    "#         print(\"Response saved.\",case)  \n",
    "        \n",
    "#     except Exception as e:\n",
    "#         error_msg = f\"\\n\\nError processing {case}:\\n{traceback.format_exc()}\"\n",
    "#         print(error_msg)\n",
    "#         failed_cases.append(case)\n",
    "\n",
    "# # Save failed cases\n",
    "# if failed_cases:\n",
    "#     with open(os.path.join(cases_dir, 'failed_deepseek-vl2.txt.txt'), 'w') as f:\n",
    "#         f.write(\"\\n\".join(failed_cases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RHUH-0019_batch_9.png']\n",
      "['/media/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized/RHUH-0019/RHUH-0019_batch_9.png']\n",
      "<image>\n",
      "[{'role': '<|User|>', 'content': \"<image>\\n<|ref|>Consider that you are a professional radiologist with several years of experience and you are now treating a patient. Write a fully detailed diagnosis report for this case, avoiding any potential hallucination and paying close attention to all of the batch images attached to this message.\\n\\nUse the following structure for the report:\\n\\n## Radiologist's Report\\n\\n### Patient Information:\\n- *Age:* 65\\n- *Sex:* Male\\n- *Days from earliest imaging to surgery:* 1\\n- *Histopathological Subtype:* Glioblastoma\\n- *WHO Grade:* 4\\n- *IDH Status:* Mutant\\n- *Preoperative KPS:* 80\\n- *Preoperative Contrast-Enhancing Tumor Volume (cm³):* 103.21\\n- *Preoperative T2/FLAIR Abnormality (cm³):* 36.29\\n- *Extent of Resection (EOR %):* 100.0\\n- *EOR Type:* Gross Total Resection (GTR)\\n- *Adjuvant Therapy:* Radiotherapy (RT) + Temozolomide (TMZ)\\n- *Progression-Free Survival (PFS) Days:* 649\\n- *Overall Survival (OS) Days:* 736\\n\\n#### Tumor Characteristics:\\n\\n#### Segmentation Analysis:\\n\\n#### Surgical Considerations:\\n\\n### Clinical Summary:\\n\\n### Recommendations:\\n\\n### Prognostic Considerations:\\n\\n### Follow-Up Plan:\\n\\n### Additional Notes*(if any)*:\\n\\nEnsure all findings from all of the images and clinical data provided. Please mention at the end of the report how many images were reviewed.\\nYou will be given batches of images, which are different sequences of MRI scans. \\n    The images are for patients who are likely to have a brain tumor. Each image will contain up to 10 slices for 5 different sequences and the segmentation masks for the tumor at the bottom row of the image. \\n    Additional clinical data about the patient is: \\n    \\n    Age: 61\\n    Sex: male\\n    Days from earliest imaging to surgery: 1\\n    Histopathological subtype: glioblastoma\\n    WHO grade: 4\\n    IDH status: wt\\n    Preoperative KPS: 60\\n    Preoperative contrast-enhancing tumor volume (cm³): 22.21\\n    Preoperative T2/FLAIR abnormality (cm³): 134.96\\n    Extent of resection (EOR %): 97.4\\n    EOR type: NTR\\n    Adjuvant therapy: RT + TMZ\\n    Progression free survival (PFS) days: 267\\n    Overall survival (OS) days: 385\\n    <|/ref|>\", 'images': ['/media/RLAB-Disk01/(final)merged_images_with_labels_order_and_folders_mask_normalized/RHUH-0019/RHUH-0019_batch_9.png']}, {'role': '<|Assistant|>', 'content': ''}]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "BatchCollateOutput(sft_format=[\"Consider that you are a professional radiologist with several years of experience and you are now treating a patient. Write a fully detailed diagnosis report for this case, avoiding any potential hallucination and paying close attention to all of the batch images attached to this message.\\n\\nUse the following structure for the report:\\n\\n## Radiologist's Report\\n\\n### Patient Information:\\n- *Age:* 65\\n- *Sex:* Male\\n- *Days from earliest imaging to surgery:* 1\\n- *Histopathological Subtype:* Glioblastoma\\n- *WHO Grade:* 4\\n- *IDH Status:* Mutant\\n- *Preoperative KPS:* 80\\n- *Preoperative Contrast-Enhancing Tumor Volume (cm³):* 103.21\\n- *Preoperative T2/FLAIR Abnormality (cm³):* 36.29\\n- *Extent of Resection (EOR %):* 100.0\\n- *EOR Type:* Gross Total Resection (GTR)\\n- *Adjuvant Therapy:* Radiotherapy (RT) + Temozolomide (TMZ)\\n- *Progression-Free Survival (PFS) Days:* 649\\n- *Overall Survival (OS) Days:* 736\\n\\n#### Tumor Characteristics:\\n\\n#### Segmentation Analysis:\\n\\n#### Surgical Considerations:\\n\\n### Clinical Summary:\\n\\n### Recommendations:\\n\\n### Prognostic Considerations:\\n\\n### Follow-Up Plan:\\n\\n### Additional Notes*(if any)*:\\n\\nEnsure all findings from all of the images and clinical data provided. Please mention at the end of the report how many images were reviewed.\\n\\n<|User|>: <image>\\n<|ref|>Consider that you are a professional radiologist with several years of experience and you are now treating a patient. Write a fully detailed diagnosis report for this case, avoiding any potential hallucination and paying close attention to all of the batch images attached to this message.\\n\\nUse the following structure for the report:\\n\\n## Radiologist's Report\\n\\n### Patient Information:\\n- *Age:* 65\\n- *Sex:* Male\\n- *Days from earliest imaging to surgery:* 1\\n- *Histopathological Subtype:* Glioblastoma\\n- *WHO Grade:* 4\\n- *IDH Status:* Mutant\\n- *Preoperative KPS:* 80\\n- *Preoperative Contrast-Enhancing Tumor Volume (cm³):* 103.21\\n- *Preoperative T2/FLAIR Abnormality (cm³):* 36.29\\n- *Extent of Resection (EOR %):* 100.0\\n- *EOR Type:* Gross Total Resection (GTR)\\n- *Adjuvant Therapy:* Radiotherapy (RT) + Temozolomide (TMZ)\\n- *Progression-Free Survival (PFS) Days:* 649\\n- *Overall Survival (OS) Days:* 736\\n\\n#### Tumor Characteristics:\\n\\n#### Segmentation Analysis:\\n\\n#### Surgical Considerations:\\n\\n### Clinical Summary:\\n\\n### Recommendations:\\n\\n### Prognostic Considerations:\\n\\n### Follow-Up Plan:\\n\\n### Additional Notes*(if any)*:\\n\\nEnsure all findings from all of the images and clinical data provided. Please mention at the end of the report how many images were reviewed.\\nYou will be given batches of images, which are different sequences of MRI scans. \\n    The images are for patients who are likely to have a brain tumor. Each image will contain up to 10 slices for 5 different sequences and the segmentation masks for the tumor at the bottom row of the image. \\n    Additional clinical data about the patient is: \\n    \\n    Age: 61\\n    Sex: male\\n    Days from earliest imaging to surgery: 1\\n    Histopathological subtype: glioblastoma\\n    WHO grade: 4\\n    IDH status: wt\\n    Preoperative KPS: 60\\n    Preoperative contrast-enhancing tumor volume (cm³): 22.21\\n    Preoperative T2/FLAIR abnormality (cm³): 134.96\\n    Extent of resection (EOR %): 97.4\\n    EOR type: NTR\\n    Adjuvant therapy: RT + TMZ\\n    Progression free survival (PFS) days: 267\\n    Overall survival (OS) days: 385\\n    <|/ref|>\\n\\n<|Assistant|>:\"], input_ids=tensor([[     0, 128821,     28,  ...,    271, 128822,     28]]), labels=tensor([[     0, 128821,     28,  ...,    271, 128822,     28]]), images=tensor([[[[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]],\n",
      "\n",
      "\n",
      "         [[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]],\n",
      "\n",
      "\n",
      "         [[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]],\n",
      "\n",
      "\n",
      "         [[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]],\n",
      "\n",
      "\n",
      "         [[[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]],\n",
      "\n",
      "          [[-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           ...,\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "           [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039]]]]],\n",
      "       dtype=torch.float16), attention_mask=tensor([[True, True, True,  ..., True, True, True]]), images_seq_mask=tensor([[False, False, False,  ..., False, False, False]]), images_spatial_crop=tensor([[[1, 4]]]), seq_lens=[1598])\n",
      "<class 'deepseek_vl2.models.processing_deepseek_vl_v2.BatchCollateOutput'>\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 55.75 MiB is free. Process 4240 has 430.89 MiB memory in use. Including non-PyTorch memory, this process has 46.41 GiB memory in use. Of the allocated memory 35.44 GiB is allocated by PyTorch, and 10.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 84\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mprepare_inputs_embeds(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessages prepared for case \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 47.50 GiB of which 55.75 MiB is free. Process 4240 has 430.89 MiB memory in use. Including non-PyTorch memory, this process has 46.41 GiB memory in use. Of the allocated memory 35.44 GiB is allocated by PyTorch, and 10.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "failed_cases = []\n",
    "\n",
    "\n",
    "for case in cases_to_process:\n",
    "    case_dir = os.path.join(cases_dir, case)\n",
    "\n",
    "    clinical_path = os.path.join(case_dir, 'diagnostic_prompt.txt')\n",
    "    if not os.path.exists(clinical_path):\n",
    "            print(f\"Missing clinical info for {case}\")\n",
    "            failed_cases.append(case)\n",
    "            continue\n",
    "            \n",
    "    clinical_info = open(clinical_path).read()\n",
    "        \n",
    "\n",
    "    system_prompt = \"\"\"Consider that you are a professional radiologist with several years of experience and you are now treating a patient. Write a fully detailed diagnosis report for this case, avoiding any potential hallucination and paying close attention to all of the batch images attached to this message.\n",
    "\n",
    "Use the following structure for the report:\n",
    "\n",
    "## Radiologist's Report\n",
    "\n",
    "### Patient Information:\n",
    "- *Age:* 65\n",
    "- *Sex:* Male\n",
    "- *Days from earliest imaging to surgery:* 1\n",
    "- *Histopathological Subtype:* Glioblastoma\n",
    "- *WHO Grade:* 4\n",
    "- *IDH Status:* Mutant\n",
    "- *Preoperative KPS:* 80\n",
    "- *Preoperative Contrast-Enhancing Tumor Volume (cm³):* 103.21\n",
    "- *Preoperative T2/FLAIR Abnormality (cm³):* 36.29\n",
    "- *Extent of Resection (EOR %):* 100.0\n",
    "- *EOR Type:* Gross Total Resection (GTR)\n",
    "- *Adjuvant Therapy:* Radiotherapy (RT) + Temozolomide (TMZ)\n",
    "- *Progression-Free Survival (PFS) Days:* 649\n",
    "- *Overall Survival (OS) Days:* 736\n",
    "\n",
    "#### Tumor Characteristics:\n",
    "\n",
    "#### Segmentation Analysis:\n",
    "\n",
    "#### Surgical Considerations:\n",
    "\n",
    "### Clinical Summary:\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "### Prognostic Considerations:\n",
    "\n",
    "### Follow-Up Plan:\n",
    "\n",
    "### Additional Notes*(if any)*:\n",
    "\n",
    "Ensure all findings from all of the images and clinical data provided. Please mention at the end of the report how many images were reviewed.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"You will be given batches of images, which are different sequences of MRI scans. \n",
    "    The images are for patients who are likely to have a brain tumor. Each image will contain up to 10 slices for 5 different sequences and the segmentation masks for the tumor at the bottom row of the image. \n",
    "    Additional clinical data about the patient is: \n",
    "    {clinical_info}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Prepare conversation\n",
    "    conversation = build_conversation(case_dir, user_prompt,system_prompt)\n",
    "    pil_images = load_pil_images(conversation)\n",
    "        \n",
    "    print(conversation)\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------------------------------------\")\n",
    "        # Process inputs\n",
    "    inputs = processor(\n",
    "            conversations=conversation,\n",
    "            images=pil_images,\n",
    "            force_batchify=True,\n",
    "            system_prompt=system_prompt\n",
    "        ).to(model.device,dtype=torch.float16)\n",
    "    \n",
    "    print(inputs)\n",
    "    print(type(inputs))\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    model = model.eval()\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "    inputs_embeds = model.prepare_inputs_embeds(**inputs)\n",
    "\n",
    "    print(f\"Messages prepared for case {case}\")\n",
    "        # Generate response\n",
    "\n",
    "\n",
    "\n",
    "    outputs = model.language.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=4096,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        \n",
    "        # Decode and save response\n",
    "\n",
    "    print(outputs.cpu().tolist())  \n",
    "    response = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Response generated for case {case}, length: {len(response)} characters\")\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    del inputs, outputs, pil_images, conversation, inputs_embeds\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model._clear_cuda_cache()\n",
    "\n",
    "    with open(os.path.join(case_dir, 'deepseek-vl.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write(response.strip())\n",
    "\n",
    "    print(\"Response saved.\",case)        \n",
    "        # Cleanup\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     error_msg = f\"\\n\\nError processing {case}:\\n{traceback.format_exc()}\"\n",
    "    #     print(error_msg)\n",
    "    #     failed_cases.append(case)\n",
    "\n",
    "# Save failed cases\n",
    "if failed_cases:\n",
    "    with open(os.path.join(cases_dir, 'failed_deepseek-vl-small.txt'), 'w') as f:\n",
    "        f.write(\"\\n\".join(failed_cases))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSeek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
