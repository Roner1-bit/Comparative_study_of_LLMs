{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZiQmu4jZ80H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "1DhIOFCo9e02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = 'path_to_root_folder'  # Replace with your root folder path"
      ],
      "metadata": {
        "id": "lC_cIhVi80fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_responses = []\n",
        "reference_responses = []"
      ],
      "metadata": {
        "id": "OLaaXaw-81xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for patient_folder in os.listdir(root_dir):\n",
        "    patient_path = os.path.join(root_dir, patient_folder)\n",
        "    if os.path.isdir(patient_path):\n",
        "        print(f\"Processing {patient_folder}...\")\n",
        "\n",
        "        # Paths to the response files\n",
        "        model_file = os.path.join(patient_path, 'model_responses.txt')\n",
        "        reference_file = os.path.join(patient_path, 'reference_responses.txt')\n",
        "\n",
        "        # Check if both files exist\n",
        "        if os.path.exists(model_file) and os.path.exists(reference_file):\n",
        "            with open(model_file, 'r') as f:\n",
        "                model_texts = f.readlines()\n",
        "            with open(reference_file, 'r') as f:\n",
        "                reference_texts = f.readlines()\n",
        "\n",
        "            # Ensure both files have the same number of responses\n",
        "            min_length = min(len(model_texts), len(reference_texts))\n",
        "            model_responses.extend(model_texts[:min_length])\n",
        "            reference_responses.extend(reference_texts[:min_length])\n",
        "        else:\n",
        "            print(f\"Warning: Missing files in {patient_folder}\")"
      ],
      "metadata": {
        "id": "zIfVrqeo83CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize metrics lists\n",
        "bleu_scores = []\n",
        "rouge1_scores = []\n",
        "rougeL_scores = []\n",
        "bert_f1_scores = []\n",
        "cosine_similarities = []\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Calculate Metrics\n",
        "for ref, pred in zip(reference_responses, model_responses):\n",
        "    # BLEU Score\n",
        "    bleu_scores.append(sentence_bleu([ref.split()], pred.split()))\n",
        "\n",
        "    # ROUGE Scores\n",
        "    rouge_scores = scorer.score(ref, pred)\n",
        "    rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
        "    rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
        "\n",
        "# Calculate BERTScore\n",
        "P, R, F1 = score(model_responses, reference_responses, lang='en', verbose=True)\n",
        "bert_f1_scores = F1.tolist()\n",
        "\n",
        "# Calculate Cosine Similarities\n",
        "all_text = reference_responses + model_responses\n",
        "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix[:len(reference_responses)], tfidf_matrix[len(reference_responses):])\n",
        "cosine_similarities = [cosine_sim_matrix[i, i] for i in range(len(reference_responses))]"
      ],
      "metadata": {
        "id": "RRdoogqy84yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = range(1, len(reference_responses) + 1)"
      ],
      "metadata": {
        "id": "TUHN6dQy867H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU Score Plot\n",
        "plt.figure()\n",
        "plt.plot(x, bleu_scores, label='BLEU Score')\n",
        "plt.title('BLEU Score Per Response')\n",
        "plt.xlabel('Response Index')\n",
        "plt.ylabel('BLEU Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# ROUGE-1 and ROUGE-L Plot\n",
        "plt.figure()\n",
        "plt.plot(x, rouge1_scores, label='ROUGE-1 F1')\n",
        "plt.plot(x, rougeL_scores, label='ROUGE-L F1')\n",
        "plt.title('ROUGE Scores Per Response')\n",
        "plt.xlabel('Response Index')\n",
        "plt.ylabel('ROUGE F1 Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# BERTScore F1 Plot\n",
        "plt.figure()\n",
        "plt.plot(x, bert_f1_scores, label='BERTScore F1')\n",
        "plt.title('BERTScore F1 Per Response')\n",
        "plt.xlabel('Response Index')\n",
        "plt.ylabel('BERTScore F1')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Cosine Similarity Plot\n",
        "plt.figure()\n",
        "plt.plot(x, cosine_similarities, label='Cosine Similarity')\n",
        "plt.title('Cosine Similarity Per Response')\n",
        "plt.xlabel('Response Index')\n",
        "plt.ylabel('Cosine Similarity')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zapx4LGZ89B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Metrics\n",
        "print(\"Average BLEU Score:\", np.mean(bleu_scores))\n",
        "print(\"Average ROUGE-1 F1:\", np.mean(rouge1_scores))\n",
        "print(\"Average ROUGE-L F1:\", np.mean(rougeL_scores))\n",
        "print(\"Average BERTScore F1:\", np.mean(bert_f1_scores))\n",
        "print(\"Average Cosine Similarity:\", np.mean(cosine_similarities))"
      ],
      "metadata": {
        "id": "K-YXxyYX8_Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming x, bleu_scores, rouge1_scores, rougeL_scores, bert_f1_scores, and cosine_similarities are already defined\n",
        "data = {\n",
        "    'Response Index': x,\n",
        "    'BLEU Score': bleu_scores,\n",
        "    'ROUGE-1 F1': rouge1_scores,\n",
        "    'ROUGE-L F1': rougeL_scores,\n",
        "    'BERTScore F1': bert_f1_scores,\n",
        "    'Cosine Similarity': cosine_similarities\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "SYYL6HK09kEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15,10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "axes[0].plot(df['Response Index'], df['BLEU Score'])\n",
        "axes[0].set_title('BLEU Score')\n",
        "axes[0].set_xlabel('Response Index')\n",
        "axes[0].set_ylabel('BLEU Score')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(df['Response Index'], df['ROUGE-1 F1'])\n",
        "axes[1].set_title('ROUGE-1 F1')\n",
        "axes[1].set_xlabel('Response Index')\n",
        "axes[1].set_ylabel('ROUGE-1 F1')\n",
        "axes[1].grid(True)\n",
        "\n",
        "axes[2].plot(df['Response Index'], df['ROUGE-L F1'])\n",
        "axes[2].set_title('ROUGE-L F1')\n",
        "axes[2].set_xlabel('Response Index')\n",
        "axes[2].set_ylabel('ROUGE-L F1')\n",
        "axes[2].grid(True)\n",
        "\n",
        "axes[3].plot(df['Response Index'], df['BERTScore F1'])\n",
        "axes[3].set_title('BERTScore F1')\n",
        "axes[3].set_xlabel('Response Index')\n",
        "axes[3].set_ylabel('BERTScore F1')\n",
        "axes[3].grid(True)\n",
        "\n",
        "axes[4].plot(df['Response Index'], df['Cosine Similarity'])\n",
        "axes[4].set_title('Cosine Similarity')\n",
        "axes[4].set_xlabel('Response Index')\n",
        "axes[4].set_ylabel('Cosine Similarity')\n",
        "axes[4].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c894hB-j9m21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_palette('Set1')\n",
        "sns.boxplot(data=df[['BLEU Score', 'ROUGE-1 F1', 'ROUGE-L F1', 'BERTScore F1', 'Cosine Similarity']])\n",
        "plt.title('Distribution of Metrics')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HzMRfwpu9okG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "averages = df.mean()\n",
        "sns.barplot(x=averages.index, y=averages.values)\n",
        "plt.title('Average Scores of Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Average Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WcLUIqkx9qBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.line(df, x='Response Index', y=['BLEU Score', 'ROUGE-1 F1', 'ROUGE-L F1', 'BERTScore F1', 'Cosine Similarity'],\n",
        "              title='Metrics Over Responses')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZekmoAgw9ro-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version 2**"
      ],
      "metadata": {
        "id": "VSCYV-_v-e-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qlOZkBpk-jB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = 'path_to_root_folder'  # Replace with your root folder path"
      ],
      "metadata": {
        "id": "rZvST6mx-kCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_responses = []\n",
        "all_reference_responses = []"
      ],
      "metadata": {
        "id": "Sai31qUq-mLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for patient_folder in os.listdir(root_dir):\n",
        "    patient_path = os.path.join(root_dir, patient_folder)\n",
        "    if os.path.isdir(patient_path):\n",
        "        print(f\"Processing {patient_folder}...\")\n",
        "\n",
        "        # Paths to the response files\n",
        "        model_file = os.path.join(patient_path, 'model_responses.txt')\n",
        "        reference_file = os.path.join(patient_path, 'reference_responses.txt')\n",
        "\n",
        "        # Check if both files exist\n",
        "        if os.path.exists(model_file) and os.path.exists(reference_file):\n",
        "            with open(model_file, 'r') as f:\n",
        "                model_texts = f.readlines()\n",
        "            with open(reference_file, 'r') as f:\n",
        "                reference_texts = f.readlines()\n",
        "\n",
        "            # Ensure both files have the same number of responses\n",
        "            min_length = min(len(model_texts), len(reference_texts))\n",
        "            model_responses = model_texts[:min_length]\n",
        "            reference_responses = reference_texts[:min_length]\n",
        "\n",
        "            # Calculate metrics for this patient\n",
        "            bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(reference_responses, model_responses)]\n",
        "            rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "            rouge1_scores = [rouge_scorer_obj.score(ref, pred)['rouge1'].fmeasure for ref, pred in zip(reference_responses, model_responses)]\n",
        "            rougeL_scores = [rouge_scorer_obj.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(reference_responses, model_responses)]\n",
        "            P, R, F1 = score(model_responses, reference_responses, lang='en', verbose=False)\n",
        "            bert_f1_scores = F1.tolist()\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            all_text = reference_responses + model_responses\n",
        "            tfidf_matrix = vectorizer.fit_transform(all_text)\n",
        "            cosine_sim_matrix = cosine_similarity(tfidf_matrix[:len(reference_responses)], tfidf_matrix[len(reference_responses):])\n",
        "            cosine_similarities = [cosine_sim_matrix[i, i] for i in range(len(reference_responses))]\n",
        "\n",
        "            # Create plots folder if it doesn't exist\n",
        "            plots_folder = os.path.join(patient_path, 'plots')\n",
        "            if not os.path.exists(plots_folder):\n",
        "                os.makedirs(plots_folder)\n",
        "\n",
        "            # Save BLEU Score Plot\n",
        "            plt.figure()\n",
        "            plt.plot(range(1, len(bleu_scores) + 1), bleu_scores)\n",
        "            plt.title('BLEU Score Per Response')\n",
        "            plt.xlabel('Response Index')\n",
        "            plt.ylabel('BLEU Score')\n",
        "            plt.savefig(os.path.join(plots_folder, 'bleu_scores.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Save ROUGE-1 and ROUGE-L Plot\n",
        "            plt.figure()\n",
        "            plt.plot(range(1, len(rouge1_scores) + 1), rouge1_scores, label='ROUGE-1 F1')\n",
        "            plt.plot(range(1, len(rougeL_scores) + 1), rougeL_scores, label='ROUGE-L F1')\n",
        "            plt.title('ROUGE Scores Per Response')\n",
        "            plt.xlabel('Response Index')\n",
        "            plt.ylabel('ROUGE F1 Score')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(plots_folder, 'rouge_scores.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Save BERTScore F1 Plot\n",
        "            plt.figure()\n",
        "            plt.plot(range(1, len(bert_f1_scores) + 1), bert_f1_scores)\n",
        "            plt.title('BERTScore F1 Per Response')\n",
        "            plt.xlabel('Response Index')\n",
        "            plt.ylabel('BERTScore F1')\n",
        "            plt.savefig(os.path.join(plots_folder, 'bert_f1_scores.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Save Cosine Similarity Plot\n",
        "            plt.figure()\n",
        "            plt.plot(range(1, len(cosine_similarities) + 1), cosine_similarities)\n",
        "            plt.title('Cosine Similarity Per Response')\n",
        "            plt.xlabel('Response Index')\n",
        "            plt.ylabel('Cosine Similarity')\n",
        "            plt.savefig(os.path.join(plots_folder, 'cosine_similarities.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Append this patient's metrics to the overall lists\n",
        "            all_model_responses.extend(model_responses)\n",
        "            all_reference_responses.extend(reference_responses)\n",
        "        else:\n",
        "            print(f\"Warning: Missing files in {patient_folder}\")"
      ],
      "metadata": {
        "id": "UpIp6Pq8-nkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute overall metrics\n",
        "overall_bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(all_reference_responses, all_model_responses)]\n",
        "rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "overall_rouge1_scores = [rouge_scorer_obj.score(ref, pred)['rouge1'].fmeasure for ref, pred in zip(all_reference_responses, all_model_responses)]\n",
        "overall_rougeL_scores = [rouge_scorer_obj.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(all_reference_responses, all_model_responses)]\n",
        "P, R, F1 = score(all_model_responses, all_reference_responses, lang='en', verbose=False)\n",
        "overall_bert_f1_scores = F1.tolist()\n",
        "vectorizer = TfidfVectorizer()\n",
        "all_text = all_reference_responses + all_model_responses\n",
        "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
        "cosine_sim_matrix = cosine_similarity(tfidf_matrix[:len(all_reference_responses)], tfidf_matrix[len(all_model_responses):])\n",
        "overall_cosine_similarities = [cosine_sim_matrix[i, i] for i in range(len(all_reference_responses))]\n",
        "\n",
        "# Create dataframe for overall metrics\n",
        "data = {\n",
        "    'BLEU Score': overall_bleu_scores,\n",
        "    'ROUGE-1 F1': overall_rouge1_scores,\n",
        "    'ROUGE-L F1': overall_rougeL_scores,\n",
        "    'BERTScore F1': overall_bert_f1_scores,\n",
        "    'Cosine Similarity': overall_cosine_similarities\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create 'llama_11b_statistics' folder if it doesn't exist\n",
        "statistics_folder = os.path.join(root_dir, 'llama_11b_statistics')\n",
        "if not os.path.exists(statistics_folder):\n",
        "    os.makedirs(statistics_folder)\n",
        "\n",
        "# Save Box Plot for Metric Distributions\n",
        "plt.figure()\n",
        "df.boxplot()\n",
        "plt.title('Distribution of Metrics')\n",
        "plt.savefig(os.path.join(statistics_folder, 'metric_distributions.png'))\n",
        "plt.close()\n",
        "\n",
        "# Save Bar Chart for Average Scores\n",
        "averages = df.mean()\n",
        "plt.figure()\n",
        "averages.plot(kind='bar')\n",
        "plt.title('Average Scores of Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Average Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.savefig(os.path.join(statistics_folder, 'average_scores.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "nGtoox4s-qTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6i3kNtCD29m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}